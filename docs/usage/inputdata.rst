Input dataset structure
=======================

If you are only working using the S3 datasets, it will not be necessary do
anything else than having access to the correct S3 buckets and either
downloading a dataset of choice or using a S3 URL as input to the program.

Working with data in AWS S3
---------------------------

The default dataset is contained in S3. Newly generated data is also expected to
be reuploaded to S3. In order to work with S3 data credentials need to be
obtained for S3.

S3 bucket overview
^^^^^^^^^^^^^^^^^^

mll-flowdata
   Raw MFC fcs data directly uploaded from MLL with hashed ids. Separate panels
   are stored as separate root-level keys in S3 with each containing a
   :file:`case_info.json` with additional metadata.

mll-sommaps
   Any data that has been generated by flowCat as in the current stage. Each
   folder contains named subdirectories that correspond to different runs.

   models
      Information and saved weights from classification models.

   output
      Classification output such as confusion matrices and raw prediction
      values.

   reference_maps
      Maps that are used to initialize node weights for training on individual
      samples.

   sample_maps
      Individual sample maps. A single sample will have one map for each single
      tube.

Setting up AWS access
^^^^^^^^^^^^^^^^^^^^^

If you are working on an EC2-instance, you should not manually add credentials
to your instance but rather enable access permissions for the specific instance
using `IAM roles`_. Otherwise follow the instructions in this section.

Credentials for AWS consist of an access key id and a secret access key. If you
have this information, a simple way to bootstrap aws is to use
:program:`awscli`. Follow instructions on `how to setup awscli`_.

:program:`awscli` can be installed on ubuntu via:

.. code-block:: sh

   sudo apt-get install awscli

Set your default region to ``eu-central-1``.

Alternatively if you have access to an already configured linux machine, you
could just copy the :file:`~/.aws` directory to the new computer.

:program:`awscli` can be used to quickly browse exiting buckets:

.. code-block:: sh

   # show list of existing buckets
   aws s3 ls

   # show folders inside of bucket
   aws s3 ls mll-flowdata/

and copy files:

.. code-block:: sh

   # copy single files
   # remote has to use complete s3 urls.
   aws s3 cp s3://mll-flowdata/CLL-9F/case_info.json ./

   # get whole folders with sync
   # caveat: this is unidirectional source -> destination
   aws s3 sync s3://mll-sommaps/models models

   # cp can be used similarly to sync with
   # difference between cp and sync, is that cp will always download all files
   # while sync will update only newer files
   aws s3 cp --recursive s3://mll-sommaps/models models

   # we can define patterns to exclude certain files
   aws s3 sync --exclude "*.png" output/models s3://mll-sommaps/models

   # whitelisting works by excluding everything first
   aws s3 sync --exclude "*" --include "*.jpeg" s3://mll-sommaps/output output

.. _IAM roles: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html
.. _how to setup awscli: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html


Working with local data
-----------------------

When working with local datasets we just have to make sure to point to the
correct locations.

When working with :class:`flowcat.data.case_dataset.CaseCollection` objects, the
input path should refer to the directory containing the :file:`case_info.json`.

For example:

.. code-block:: sh

   example_dataset
   ├── case_info.json
   ├── CLL
   └── normal

:file:`example_dataset` would refer to our dataset. This can be loaded in a
python script using the :mod:`flowcat.data.case_dataset` methods:
::

   from flowcat.data.case_dataset import CaseCollection
   cases = CaseCollection("example_dataset")
